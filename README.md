# Margin Maximization in Attention Mechanism
This repository holds the official code for the paper [Max-Margin Token Selection in Attention Mechanism](https://arxiv.org/pdf/2306.13596v1.pdf)

### Abstract
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. We explore the seminal softmax-attention model 
$$f(X)=v^\top X^\top \texttt{softmax}(X W^\top p),$$
where, $X$ is the tokenized input, $v$ is the value weights, $W$ is the key-query weights, and $p$ is a tunable token/prompt. We show that running gradient descent on $p$, or equivalently $W$, converges to a max-margin solution that separates locally-optimal tokens from non-optimal ones. When optimizing $v$ and $p$ simultaneously with logistic loss, we identify conditions under which the regularization paths converge to their respective max-margin solutions where $v$ separates the input features based on their labels.  

### Experimental Details
We implemented an attention layer using PyTorch. During training, we utilized the SGD optimizer with a learning rate of 0.1 and trained the model for 1000 iterations. To visualize the generalization path more effectively, we normalized the gradient of the variables $p$ and $v$ at each iteration.

Once we obtained the solution, denoted as $\hat{p}$ (p-hat), we determined the locally optimal indices as those with the highest softmax scores. To solve the optimization problem described in equation (attnsvm), we used the Python package cvxopt. By solving this problem, we obtained the solution ùëù. Subsequently, we verified that these indices satisfied our local-optimal definition.

In our paper, the examples we used were constructed in such a way that it was straightforward to verify their optimality. In the joint optimization of $(p,v)$, the v-optimization was solved using the Python package sklearn.svm.

### Requirements 



### Reproducing Results 
